{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83d1eb9",
   "metadata": {},
   "source": [
    "Text Preparation\n",
    "\n",
    "Standardizes text format for consistent processing\n",
    "\n",
    "Handles punctuation separation to improve token boundaries\n",
    "\n",
    "Expands contractions to canonical forms\n",
    "\n",
    "Reduces vocabulary size and improves model generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075ebd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello! This is a test. Don't you think it's cool?\n",
      "Prepared: hello ! this is a test . do not you think it is cool ?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def prepare_text(text):\n",
    "\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters (keep alphanumeric and basic punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?\\'\"-]', '', text)\n",
    "    \n",
    "    # Add space around punctuation for better tokenization\n",
    "    text = re.sub(r'([.,;:!?])', r' \\1 ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Expand common contractions\n",
    "    contractions = {\n",
    "        \"n't\": \" not\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'d\": \" would\"\n",
    "    }\n",
    "    for cont, exp in contractions.items():\n",
    "        text = text.replace(cont, exp)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Hello! This is a test. Don't you think it's cool?\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Prepared:\", prepare_text(sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d643eb6",
   "metadata": {},
   "source": [
    "Tokenization & Byte Pair Encoding (BPE)\n",
    "\n",
    "BPE Algorithm: Iteratively merges most frequent character pairs\n",
    "\n",
    "Vocabulary Building: Starts with byte-level tokens, grows to specified size\n",
    "\n",
    "Handling Unknowns: Uses <unk> token for unseen subwords\n",
    "\n",
    "Efficiency: Processes text in linear time relative to vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448ed66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t']\n",
      "Token IDs: [98, 114, 111, 119, 110, 102, 111, 120, 106, 117, 109, 112, 115]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, corpus=None, vocab_size=1000):\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        if corpus:\n",
    "            self.train(corpus, vocab_size)\n",
    "    \n",
    "    def train(self, corpus, vocab_size):\n",
    "        \n",
    "        # Initialize vocabulary with bytes\n",
    "        self.vocab = {chr(i): i for i in range(256)}\n",
    "        text = prepare_text(corpus)\n",
    "        words = text.split()\n",
    "        word_freqs = Counter(words)\n",
    "        \n",
    "        # BPE training algorithm\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            # Count all adjacent pairs\n",
    "            pairs = Counter()\n",
    "            for word, freq in word_freqs.items():\n",
    "                symbols = list(word)\n",
    "                for i in range(len(symbols)-1):\n",
    "                    pairs[(symbols[i], symbols[i+1])] += freq\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            new_token = ''.join(best_pair)\n",
    "            \n",
    "            # Merge the pair\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            \n",
    "            # Update word frequencies with merged token\n",
    "            new_word_freqs = {}\n",
    "            for word, freq in word_freqs.items():\n",
    "                new_word = word\n",
    "                for pair, merge in self.merges.items():\n",
    "                    new_word = new_word.replace(''.join(pair), merge)\n",
    "                new_word_freqs[new_word] = freq\n",
    "            word_freqs = new_word_freqs\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        text = prepare_text(text)\n",
    "        words = text.split()\n",
    "        \n",
    "        for word in words:\n",
    "            # Start with individual characters\n",
    "            current = list(word)\n",
    "            changed = True\n",
    "            \n",
    "            # Apply merges until no more changes\n",
    "            while changed and len(current) > 1:\n",
    "                changed = False\n",
    "                for i in range(len(current)-1):\n",
    "                    pair = (current[i], current[i+1])\n",
    "                    if pair in self.merges:\n",
    "                        current = current[:i] + [self.merges[pair]] + current[i+2:]\n",
    "                        changed = True\n",
    "                        break\n",
    "            \n",
    "            tokens.extend(current)\n",
    "        \n",
    "        # Convert tokens to IDs\n",
    "        return [self.vocab.get(token, self.vocab.get('<unk>', 0)) for token in tokens]\n",
    "\n",
    "# Example usage\n",
    "corpus = \"The quick brown fox jumps over the lazy dog repeatedly.\"\n",
    "tokenizer = BPETokenizer(corpus, vocab_size=200)\n",
    "print(\"Vocabulary:\", list(tokenizer.vocab.keys())[:10])\n",
    "print(\"Token IDs:\", tokenizer.tokenize(\"brown fox jumps\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d10f70",
   "metadata": {},
   "source": [
    "Sliding Window Sampling\n",
    "\n",
    "Sliding Window: Creates overlapping context windows\n",
    "\n",
    "Configurable Context: Adjust window_size for model context length\n",
    "\n",
    "Efficient Sampling: Uses stride to control overlap between samples\n",
    "\n",
    "PyTorch Integration: Compatible with DataLoader for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f791ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 14 examples\n",
      "First sample: tensor([ 97, 113, 117, 105,  99])\n",
      "Second sample: tensor([117, 105,  99, 107,  98])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, window_size=64, stride=32):\n",
    "\n",
    "        self.tokens = tokenizer.tokenize(text)\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def __len__(self):\n",
    "    \n",
    "        return (len(self.tokens) - self.window_size) // self.stride\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        start = idx * self.stride\n",
    "        end = start + self.window_size\n",
    "        return torch.tensor(self.tokens[start:end])\n",
    "\n",
    "# Example usage\n",
    "dataset = TextDataset(\n",
    "    text=\"A quick brown fox jumps over the lazy dog.\",\n",
    "    tokenizer=tokenizer,\n",
    "    window_size=5,\n",
    "    stride=2\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(\"First sample:\", dataset[0])\n",
    "print(\"Second sample:\", dataset[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f83bb",
   "metadata": {},
   "source": [
    " Token Vectorization\n",
    "\n",
    "Embedding Layer: Trainable lookup table for token representations\n",
    "\n",
    "Dimensionality: embedding_dim controls vector size (typical values: 128-1024)\n",
    "\n",
    "Gradient Learning: Embeddings improve during model training\n",
    "\n",
    "Output: 3D tensor (batch_size, sequence_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff793b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([102, 111, 120, 106, 117, 109, 112, 115])\n",
      "Vector shape: torch.Size([8, 128])\n",
      "First vector: [1.385013461112976, 0.023550674319267273, -0.28727394342422485, 0.025423569604754448, -1.40799081325531] ...\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "embedding_dim = 128\n",
    "embedding_layer = EmbeddingLayer(vocab_size, embedding_dim)\n",
    "\n",
    "sample_tokens = torch.tensor(tokenizer.tokenize(\"fox jumps\"))\n",
    "vectors = embedding_layer(sample_tokens)\n",
    "\n",
    "print(\"Token IDs:\", sample_tokens)\n",
    "print(\"Vector shape:\", vectors.shape)\n",
    "print(\"First vector:\", vectors[0][:5].tolist(), \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21365b77",
   "metadata": {},
   "source": [
    "Full Pipeline Integration\n",
    "\n",
    "Corpus Preparation: Real-world text with technical terms\n",
    "\n",
    "Tokenization: BPE handles complex words like \"representations\"\n",
    "\n",
    "Batching: DataLoader creates mini-batches for efficient training\n",
    "\n",
    "Vector Conversion: Tokens become trainable embedding vectors\n",
    "\n",
    "Output Shapes:\n",
    "\n",
    "Token IDs: [batch_size, window_size]\n",
    "\n",
    "Embeddings: [batch_size, window_size, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a6cd82",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m corpus = \u001b[33m\"\"\"\u001b[39m\u001b[33mLarge language models require carefully prepared text data. \u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mTokenization converts text into smaller units called tokens. \u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mByte pair encoding creates efficient subword representations.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Train BPE tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m tokenizer = \u001b[43mBPETokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 3. Create sliding window dataset\u001b[39;00m\n\u001b[32m     13\u001b[39m dataset = TextDataset(\n\u001b[32m     14\u001b[39m     text=corpus,\n\u001b[32m     15\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     16\u001b[39m     window_size=\u001b[32m10\u001b[39m,\n\u001b[32m     17\u001b[39m     stride=\u001b[32m5\u001b[39m\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mBPETokenizer.__init__\u001b[39m\u001b[34m(self, corpus, vocab_size)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.merges = {}\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m corpus:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mBPETokenizer.train\u001b[39m\u001b[34m(self, corpus, vocab_size)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Find most frequent pair\u001b[39;00m\n\u001b[32m     31\u001b[39m best_pair = \u001b[38;5;28mmax\u001b[39m(pairs, key=pairs.get)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m new_token = \u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_pair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Merge the pair\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.merges[best_pair] = new_token\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# 1. Prepare training corpus\n",
    "corpus = \"\"\"Large language models require carefully prepared text data. \n",
    "Tokenization converts text into smaller units called tokens. \n",
    "Byte pair encoding creates efficient subword representations.\"\"\"\n",
    "\n",
    "# 2. Train BPE tokenizer\n",
    "tokenizer = BPETokenizer(corpus, vocab_size=300)\n",
    "\n",
    "# 3. Create sliding window dataset\n",
    "dataset = TextDataset(\n",
    "    text=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    window_size=10,\n",
    "    stride=5\n",
    ")\n",
    "\n",
    "# 4. Create embedding layer\n",
    "embedding_layer = EmbeddingLayer(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    embedding_dim=128\n",
    ")\n",
    "\n",
    "# 5. Process through full pipeline\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(f\"\\nBatch {i+1}:\")\n",
    "    print(\"Token IDs:\", batch)\n",
    "    print(\"Shape:\", batch.shape)\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    embeddings = embedding_layer(batch)\n",
    "    print(\"Embedding shape:\", embeddings.shape)\n",
    "    \n",
    "    if i == 1:  # Show first 2 batches\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
